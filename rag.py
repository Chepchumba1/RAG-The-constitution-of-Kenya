# -*- coding: utf-8 -*-
"""RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iyigA_j5BBUFrqu_w6MRILZQvGWH5CUI
"""

!pip install langchain_community langchainhub chromadb langchain langchain-groq

!pip install -q langchain langchain-community faiss-cpu sentence-transformers pypdf

from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_groq import ChatGroq
import os

from google.colab import files
uploaded = files.upload()

pdf_path = "/content/The_Constitution_of_Kenya_2010.pdf"
loader = PyPDFLoader(pdf_path)
documents = loader.load()

#Split into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
docs = splitter.split_documents(documents)

#Create free embeddings (using MiniLM)
embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

#Store in FAISS vector database
vectorstore = FAISS.from_documents(docs, embeddings)

os.environ["GROQ_API_KEY"] = "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"

llm = ChatGroq(
    model="llama-3.1-8b-instant",
    temperature=0,
    api_key=os.environ["GROQ_API_KEY"]
)

#  Build RAG pipeline
qa = RetrievalQA.from_chain_type(
    llm=llm,#uses Groqâ€™s LLM to combine the chunks + user query into a final answer.
    retriever=vectorstore.as_retriever(search_kwargs={"k":3}),#pulls the top 3 relevant Constitution chunks.
    chain_type="stuff"
)

#  Test it
query = "What is the role of the chief justice?"
answer = qa.run(query)
print("Q:", query)
print("A:", answer)